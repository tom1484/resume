export const publications = [
  {
    title: "Learning Skills from Action-Free Videos",
    authors: ["Hung-Chieh Fang*", "Kuo-Han Hung*", "!Chu-Rong Chen", "Po-Jung Chou", "Chun-Kai Yang", "et al."],
    publication: {
      conference: "ICLR",
      status: "Under Review",
    },
    time: "2026",
    // link: [
    //   { text: "Paper", url: "" },
    // ],
    // content: [
    //   "Developed Skill Abstraction from OpticalFlow (SOF), a framework that learns reusable robotic skills directly from action-free videos using optical flow as a proxy for actions.",
    //   "Enabled multitask learning and long-horizon planning by extracting and composing visual skills without requiring action-labeled data.",
    //   "Demonstrated improved performance across diverse tasks, showing the framework's effectiveness in learning from raw video data for generalist robot behavior.",
    // ],
    // tags: ["Video Understanding", "Skill Learning", "Optical Flow", "Multitask Learning", "Long-Horizon Planning"],
  },
];

export const publications = [
  {
    title: "Learning Skills from Action-Free Videos",
    authors: ["Hung-Chieh Fang*", "Kuo-Han Hung*", "!Chu-Rong Chen", "Po-Jung Chou", "Chun-Kai Yang", "Po-Chen Ko", "Yu-Chiang Frank Wang", "Yueh-Hua Wu", "Min-Hung Chen", "Shao-Hua Sun"],
    publication: {
      conference: "ICLR 2026",
      status: "Under Review",
    },
    time: "May 2025",
    link: [
      { text: "Paper", url: "" },
    ],
    content: [
      "Developed Skill Abstraction from Optical Flow (SOF), a framework that learns reusable robotic skills directly from action-free videos using optical flow as a proxy for actions.",
      "Enabled multitask learning and long-horizon planning by extracting and composing visual skills without requiring action-labeled data.",
      "Demonstrated improved performance across diverse tasks, showing the framework's effectiveness in learning from raw video data for generalist robot behavior.",
    ],
    tags: ["Video Understanding", "Skill Learning", "Optical Flow", "Multitask Learning", "Long-Horizon Planning"],
  },
];
